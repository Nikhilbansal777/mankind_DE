âœ… Justification as Simple AI (Current System)

My system uses content-based filtering, a foundational technique in AI:
  - The AIEngine mimics intelligent behavior by recommending products from the same category as the viewed one.
  - This rule-based logic is a form of narrow AI, where intelligence is simulated through programmed rules rather than learning.
  - The StreamSimulator adds realism by generating real-time user interaction and dynamically responding with relevant outputs.

ðŸ”¹ This is weak AI (also known as narrow AI) â€” task-specific, deterministic logic that mimics human-like decisions in a limited domain.

___________________________________________________________________________________________________________________________________________

ðŸš€ Expansion into Real (Strong) AI

To evolve from rule-based AI into a more advanced AI system, you can integrate:
  - Collaborative Filtering or Machine Learning Models
  - Recommend based on user-product interaction history, not just product category.
  - Use models like k-NN, SVD, or matrix factorization.
  - NLP for Semantic Product Similarity
  - Analyze product descriptions with TF-IDF, BERT, or sentence embeddings.
  - Recommend based on semantic similarity, not just labels.
  - User Profiles and Personalization
  - Track user behavior over time to build individualized recommendation models.
  - Reinforcement Learning
  - Learn from user clicks or purchases over time to optimize recommendations dynamically.


Transitioning from a rule-based AI system to a model-based AI system involves several key prerequisites in both data infrastructure and machine learning setup.

1. DATABASE REQUIREMENTS
------------------------
To store and retrieve dynamic data related to users, products, and their interactions, the following tables are recommended:

- users: Stores user IDs and optional profile data
- products: Holds product metadata (id, name, category, description, price, etc.)
- user_events: Logs user interactions such as views, clicks, and purchases
- recommendations (optional): Caches generated recommendations for faster retrieval

2. DATA COLLECTION PIPELINE
---------------------------
To train a model, interaction data needs to be captured and stored in a structured format:
- Log events like "product_viewed", "product_clicked"
- Store interactions with timestamps
- This historical interaction data forms the basis for training machine learning models

3. ML-READY DATASET FORMAT
--------------------------
For training purposes, prepare data in formats such as:

Example 1:
user_id | product_id | event_type | timestamp  
123     | 7          | view       | 2025-05-06 09:13:00

Example 2 (for collaborative filtering):
user_id | product_id | rating (e.g., 1 if the user viewed or clicked)

These datasets can be saved as CSV files or loaded into Pandas for processing and training.

4. ML MODEL PREREQUISITES
-------------------------
Depending on the AI method used, the following prerequisites apply:

- Collaborative Filtering (e.g., Matrix Factorization):
  * Input: A sparse matrix of users Ã— products
  * Libraries: scikit-learn, surprise, lightfm

- NLP-Based Recommendations (e.g., BERT, TF-IDF):
  * Input: Product descriptions and other text data
  * Libraries: scikit-learn, transformers, spaCy, gensim

5. BACKEND INTEGRATION
----------------------
After training the model:
- Save it using tools such as joblib or pickle
- Expose the model through a RESTful API using Flask, FastAPI, or Node.js
- Integrate it into the main system via batch processing or real-time calls to generate recommendations during user interaction

SUMMARY OF PREREQUISITES
------------------------
Area             | Requirements
-----------------|----------------------------------------------
Data Storage     | MySQL or PostgreSQL for products and events
Data Logging     | Real-time logging of user interactions
ML Pipeline      | Export â†’ Preprocess â†’ Train
Libraries        | pandas, scikit-learn, lightfm, transformers
Serving          | REST API to deliver recommendations
